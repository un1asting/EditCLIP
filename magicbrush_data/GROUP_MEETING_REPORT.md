# EditCLIP vs LLM 评估方法对比
## 组会报告

**日期**: 2025-11-07
**数据**: MagicBrush 30样本
**目的**: 对比两种图像编辑评估方法的差异

---

## 一、评估方法对比

| 维度 | EditCLIP | LLM (Claude) |
|------|----------|--------------|
| **原理** | CLIP嵌入空间的相似度 | 视觉语言模型直接推理 |
| **速度** | 10ms/样本 | 3-5s/样本 |
| **成本** | 免费 | ~$0.01/样本 |
| **输出** | 单一分数 (0-1) | 多维评分 + 推理 |

---

## 二、EditCLIP 整体表现

- **样本数**: 30
- **平均分**: 0.2145 ± 0.0496
- **分布**: 大部分集中在 0.15-0.25

**关键发现**: EditCLIP 对所有编辑都给出偏保守的分数

---

## 三、预期差异案例分析

### 类型 A: EditCLIP 低分 → LLM 可能高分

#### 案例 1: 场景大幅变化
```
ID: 21
指令: "change the grass to water"
EditCLIP分数: 0.113 (最低分)

为什么 EditCLIP 低分？
- 草地→水面：纹理、颜色、反射完全改变
- 在特征空间中距离很远
- EditCLIP 认为"变化太大"

为什么 LLM 可能高分？
- 如果水面渲染真实、覆盖了原来的草地区域
- 指令语义正确执行
- LLM 理解"大变化 = 正确执行复杂指令"
```

#### 案例 2: 移除操作
```
ID: 18
指令: "Get rid of all the people"
EditCLIP分数: 0.116 (第二低)

为什么 EditCLIP 低分？
- 移除人物导致场景整体视觉特征大变
- 前景空缺，构图改变
- EditCLIP 无法理解"移除"的语义

为什么 LLM 可能高分？
- 只需检查：人物是否全部消失？
- 如果消失且背景修补自然 → 完美执行
- LLM 能理解"少了什么"也是正确的编辑
```

#### 案例 3: 背景替换
```
ID: 24
指令: "The background should be of a mountain"
EditCLIP分数: 0.159

为什么 EditCLIP 低分？
- 背景从平地/城市 → 山脉
- 大范围像素改变
- 整体色调、纹理变化

为什么 LLM 可能高分？
- 检查背景是否确实是山
- 前景主体是否保留
- 合成是否自然
```

---

### 类型 B: 两者可能都高分

#### 案例 4: 简单颜色变化
```
ID: 19
指令: "Make the umbrellas blue"
EditCLIP分数: 0.298 (最高分)

为什么都会高分？
- 颜色变化在CLIP特征空间中容易识别
- 视觉上明确且局部
- LLM 也能轻松验证"伞是否变蓝"
- 两种方法都能准确评估
```

---

### 类型 C: EditCLIP 中等分 → LLM 可能更极端

#### 案例 5: 对象替换
```
ID: 0
指令: "change the table for a dog"
EditCLIP分数: 0.180 (中等偏低)

可能的分歧：
✓ 如果狗的位置、大小、姿态都合理
  → LLM 给高分 (8-9/10)
  → EditCLIP 因整体构图变化仍给中等分

✗ 如果狗看起来像贴上去的、不自然
  → LLM 给低分 (3-4/10)
  → EditCLIP 可能因视觉特征匹配仍给中等分
```

---

## 四、差异的根本原因

### EditCLIP 的评估逻辑
```
问题: "编辑后的图像对在特征空间中是否接近指令？"

局限:
1. 大幅度视觉变化 = 特征距离远 = 低分
   即使语义正确

2. 无法区分"正确的大变化"和"错误的大变化"

3. 依赖训练数据中见过的编辑类型
```

### LLM 的评估逻辑
```
问题: "指令的语义要求是否被正确执行？"

优势:
1. 理解"移除"、"替换"、"变成"等复杂语义

2. 能判断"变化虽大，但正确"

3. 多维度评估：
   - 指令执行了吗？
   - 质量好吗？
   - 其他地方保留了吗？
```

---

## 五、预期相关性

### 整体相关性
- **Pearson 相关系数**: 0.5-0.7 (中等正相关)

### 分层相关性
| 编辑类型 | 预期相关性 | 原因 |
|---------|-----------|------|
| 颜色变化 | **高** (>0.8) | 两者都能准确识别 |
| 对象添加/替换 | 中高 (0.6-0.8) | 取决于视觉真实度 |
| **移除操作** | **低** (0.3-0.5) | EditCLIP惩罚大变化 |
| **背景变化** | **低** (0.3-0.5) | 同上 |

---

## 六、结论与建议

### 核心结论
1. **EditCLIP**: 擅长判断"看起来像不像"
   - 对简单编辑准确
   - 对大变化编辑有系统性偏见（偏低）

2. **LLM**: 擅长理解"做对了没有"
   - 深度语义理解
   - 可解释性强
   - 但慢且贵

3. **两者互补，不是替代**

### 使用建议
```
研究场景:
├── 主要指标: EditCLIP (基准测试)
├── 补充分析: LLM (错误分析)
└── 重点关注: 两者分歧大的案例

实际应用:
├── EditCLIP: 快速筛选 (1000+ 样本)
├── LLM: 精细验证 (关键样本)
└── 人工: 最终决策 (边界情况)
```

### 后续工作
- [ ] 运行 LLM 评估验证假设
- [ ] 计算实际相关性
- [ ] 分析具体差异案例
- [ ] 探索混合评估方法

---

## 七、典型差异模式总结图

```
编辑复杂度 vs 评估分歧

分歧程度
  ↑
  │         ⚠ 移除操作
  │      ⚠ 背景替换
  │   ⚠ 场景变换
  │
  │ ✓ 对象替换
  │✓ 颜色变化
  │✓ 属性添加
  └───────────────→ 视觉变化幅度

✓ = 两者一致
⚠ = 显著分歧 (EditCLIP偏低，LLM可能高)
```

---

## 八、核心要点 (30秒版)

1. **EditCLIP** 平均分 0.21，对大变化编辑评分偏低
2. **关键差异**: 移除/背景变化类编辑，EditCLIP低分(~0.11)但可能执行正确
3. **原因**: EditCLIP看"视觉相似度"，LLM懂"语义正确性"
4. **建议**: EditCLIP做快速筛选，LLM做深入分析，两者结合

---

**报告结束**
*详细分析见 EVALUATION_COMPARISON_REPORT.md*
